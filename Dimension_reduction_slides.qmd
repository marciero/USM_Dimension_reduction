---
title: "Dimension Reduction"
subtitle: "A Quick Tour with Examples"
author: "USM Data Science"
date: "2026-02-04"
title-slide-attributes:
  data-background-color: "#002752"
format: 
    revealjs:
        footer: "Dimension Reduction/USM 2026-02-04"
        html-math-method: mathjax
        slide-number: true
        menu: false
        freeze: auto
        embed-resources: true
editor_options: 
  
  chunk_output_type: console
execute:
  echo: false
  warning: false
  message: false
  error: false
---

```{r echo=FALSE, warning=FALSE, include=FALSE}

library(tidyverse)
library(tidymodels)
library(palmerpenguins)
library(faraway)

library(broom)
library(GGally)
library(pscl)
library(embed)
library(patchwork)
library(kableExtra)
library(scales)
library(latex2exp)
library(tidytext)




```

```{r knitr, warning=FALSE}
options(htmltools.dir.version = FALSE, digits = 3)
knitr::opts_chunk$set(
  fig.width=7.5, fig.height=4.2, fig.retina=3,
  out.width = "100%",
  cache = FALSE,
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  fig.show = TRUE,
  hiline = TRUE,
  require("knitr")
)

theme_set(
  theme_minimal(base_size = 11) +
    theme(
      plot.title   = element_text(size = 20),
      axis.title   = element_text(size = 16),
      axis.text    = element_text(size = 14),
      legend.title = element_text(size = 16),
      legend.text  = element_text(size = 12)
    )
)
```

## Class resources 


<br>

- [Github repo Dimension reduction](https://github.com/marciero/USM_Dimension_reduction.git)

- [Colab notebooks](https://drive.google.com/drive/folders/18yDv-ZO1pgPc1XvatsoHW1OkM6Q1RWVU?usp=sharing)

- [R Posit Cloud R project](https://positcloud.com/marciero/projects)


# What is Dimension Reduction?

## Consider a typical machine learning situation {.incremental}

::: fragment
-   one **response** variable (or none - unsupervised)
-   one or more **predictors** (features, variables)
:::

::: fragment
`penguins` data[^1].

```{r}
penguins <- penguins[sample(1:nrow(penguins)), ]  

head(penguins) %>% kable() %>%
  kable_styling(full_width = F, position = "left", font_size = 20)

```
:::

[^1]: Palmer Archipelago, Antarctica. [Gorman, et. al, (2014)](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0090081), R package `palmerpenguins`, Horst et al.

:::: fragment
We are interested in the dimension of the predictor space.

::: {.callout-tip title="Think linear algebra"}
Dimension of the **column space**
:::
::::

## An Example {.incremental}

[Sakar et al. (2019)](https://doi.org/10.1016/j.asoc.2018.10.022)

-   **252 patients**, 188 of whom had a previous diagnosis of Parkinson’s disease, 64 without.
-   speech signal processing techniques yield **750 numerical predictors**, or *features*.

<br>

Goal: classify Parkinson’s status.

<br>

:::: fragment
::: {.callout-note appearance="simple"}
Can we fit a logistic regression model?
:::
::::

## The `parkinsons` design matrix {.incremental}

-   751 rows and 252 + 1 = 253 columns (one for the intercept)

-   More columns than rows!

-   So data matrix $X$ not *full rank* - a problem for some ML models

-   $p>>n$ is common in many ML applications (e.g., genomics, image analysis, text mining, etc.)

## Feature selection - one form of dimension reduction

Even if $X$ full rank, we might want eliminate some predictors to decrease model *variance*.

::: incremental
-   near - zero variance predictors

-   highly correlated predictors (with each other)

-   predictors uncorrelated with the response
:::

::: fragment
Many methods...
:::

## Another approach: Feature engineering {.small .incremental}

:::::: fragment
::::: columns
::: {.column width="50%"}
```{r}

#ggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))

set.seed(123)
df <- tibble(x = runif(100, 0, 10)) %>% 
  mutate(y = x + rnorm(100, 0, 1)) 

df %>% 
ggplot(aes(x = x, y = y)) +
  geom_point() +
    geom_hline(yintercept = 0, linewidth = 1) +
  geom_vline(xintercept = 0, linewidth = 1) +
  theme(axis.text.y = element_blank(), 
        axis.text.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        #panel.grid = element_blank()
        )+
    labs(x = TeX("$X_1$"), 
       y = TeX("$X_2$"),
    title = "Two predictors living mostly in 1 D space")
```
:::

::: {.column width="50%"}
```{r}
df %>% 
ggplot(aes(x = x, y = y)) +
  geom_point() +
    geom_hline(yintercept = 0, linewidth = 1, alpha = 0.2) +
  geom_vline(xintercept = 0, linewidth = 1, alpha = 0.2) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed", linewidth = 1, color = "red") +
    geom_segment(
    x = 5, y = 5,
    xend = 4.5, yend = 6.5,
    linewidth = 1,
    color = "red",
    linetype = "dashed",
  ) +
  theme(axis.text.y = element_blank(), 
        axis.text.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        #panel.grid = element_blank()
        )+
    labs(x = NULL, 
       y = NULL,
    title = TeX("New features: $X_1 + X_2$ and $X_2 - X_1$"))
```
:::
:::::
::::::

:::::: fragment
::::: columns
::: {.column width="50%"}
```{r}

#ggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))

set.seed(123)
df <- tibble(x = runif(100, 0, 10)) %>% 
  mutate(y = (x - 5)^2  + rnorm(100, 0, 3)) %>% 
    mutate(xsq = x^2  + rnorm(100, 0, 3))

df %>% 
ggplot(aes(x = x, y = y)) +
  geom_point() +
    geom_hline(yintercept = 0, linewidth = 1) +
  geom_vline(xintercept = 0, linewidth = 1) +
  theme(axis.text.y = element_blank(), 
        axis.text.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        #panel.grid = element_blank()
        )+
    labs(x = TeX("$X_1$"), 
       y = TeX("$X_2$"), 
    title = "Two predictors living mostly in 1 D space")
```
:::

::: {.column width="50%"}
```{r}
df %>% 
ggplot(aes(x = x, y = xsq)) +
  geom_point() +
    geom_hline(yintercept = 0, linewidth = 1, alpha = 0.2) +
  geom_vline(xintercept = 0, linewidth = 1, alpha = 0.2) +
    geom_smooth(method = "lm", se = FALSE, linewidth = 1, linetype = "dashed") +
  theme(axis.text.y = element_blank(), 
        axis.text.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        #panel.grid = element_blank()
        )+
    labs(x = NULL, 
       y = NULL,
       title = TeX("New feature: $X_1^2$"))
  
```
:::
:::::
::::::

:::: fragment
::: {callout-tip}
More generally, can we transform and combine predictors in various ways to form a smaller set of new *features?*
:::
::::

## The goal of dimension reduction 



## Why do dimension reduction? {.incremental}

::: incremental
-   Reduce model variance - mitigate **multicollinearity**
-   Reduce model complexity
-   Improve model interpretability
-   Reduce computational cost
-   Mitigate the **"curse of dimensionality"**
:::

:::: fragment
::: {.callout-note appearance="simple"}
Much of this is model dependent
:::
::::

::: fragment
Also used for

-   Visualize high-dimensional data
-   data compression
-   autoencoders - representation learning
:::

## Curse of dimensionality

::::: columns
::: {.column width="50%"}
![](images/screaming_cropped-scaled.jpg)
:::

::: {.column .middle width="50%"}
<br>

<br>

[`sklearn NearestNeighbors()`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html)
:::
:::::

## Curse of dimensionality

Suppose KNN with $p$ predictors $\sim U(0,1)$

-   volume of hypercube with sides of length $d$ is $d^p$.

-   To capture $r$ proportion of the data, we want $d^p = r$, so $d = r^{1/p}$

```{r}
p <- c(1:100)
r <- c(0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 0.5)


df <- expand.grid(p = p, r = r) %>%
    mutate(d = r^(1/p),
           r_lab = if_else(r < 0.1,
                           paste0("10^{", format(log10(r), trim = TRUE), "}"),
                           as.character(r))) %>%
    mutate(r_lab = factor(r_lab, levels = unique(r_lab)))  # preserve order

ggplot(df, aes(x = p, y = d, color = r_lab)) +
    geom_line() +
    geom_point() +
    labs(x = "Number of predictors, p", y = "Box side length, d") +
    scale_color_discrete(name = "Proportion of data, r", labels = label_parse()) +
    theme_minimal()

```

::: {.callout-note appearance="simple"}
We are using the $L_{\infty}$ norm (max norm) to define neighborhood here, but argument is similar for other norms.
:::

## Feature selection/engineering/extraction and Dimension reduction {.small}

**Feature "selection"**

-   select a subset of existing predictors or features

**Feature engineering**

-   create new predictors or features from existing ones- transformations, combinations, interactions, polynomials, etc.

**Feature extraction**

-   create a lower dimensional representation that is structurally similar to the original predictors

# Principal Components Analysis (PCA) {.small}

A Canonical Example

## Principal Components Analysis (PCA) {.small}

Create features - *components* from **linear combinations** of the predictors

::::: columns
::: {.column width="50%"}
```{r}

#ggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))

set.seed(123)
df <- tibble(x = runif(100, 0, 10)) %>% 
  mutate(y = x + rnorm(100, 0, 1)) 

df %>% 
ggplot(aes(x = x, y = y)) +
  geom_point() +
    geom_hline(yintercept = 0, linewidth = 1) +
  geom_vline(xintercept = 0, linewidth = 1) +
  theme(axis.text.y = element_blank(), 
        axis.text.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        #panel.grid = element_blank()
        )+
    labs(x = "X1", 
       y = "X2",
    title = "Two predictors living mostly in 1 D space")
```
:::

::: {.column width="50%"}
```{r}
df %>% 
ggplot(aes(x = x, y = y)) +
  geom_point() +
    geom_hline(yintercept = 0, linewidth = 1, alpha = 0.2) +
  geom_vline(xintercept = 0, linewidth = 1, alpha = 0.2) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed", linewidth = 1, color = "red") +
    geom_segment(
    x = 5, y = 5,
    xend = 4.5, yend = 6.5,
    linewidth = 1,
    color = "red",
    linetype = "dashed",
  ) +
  theme(axis.text.y = element_blank(), 
        axis.text.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        #panel.grid = element_blank()
        )+
    labs(x = NULL, 
       y = NULL,
    title = TeX("New features: $a X_1 + b X_2$ and $c X_2 - d X_1$"))
```
:::
:::::

-   The first component is in the direction of maximum variance of the data\*.

-   Second component in direction of max variance orthogonal (and thus uncorrelated) to the first, and so on.

::: {.callout-caution appearance="simple"}
\*It thus important to scale predictors first if they are on different scales.
:::

:::: fragment
::: {.callout-note appearance="simple"}
PCA is **unsupervised**!
:::
::::

## PCA - the math {.incremental}

Predictors $X_1, X_2, \dots, X_p$; form linear combinations

$$
Z_m = \sum_{j=1}^p \phi_{jm} X_j,
\qquad m = 1, 2, \dots, p
$$

In matrix form,

$$
Z = X \mathbf{U}
$$

Now with data

$$
\mathbf{Z} = \mathbf{XU}
$$

## Principal components and eigenvectors

First scale or standardize the data matrix.

The **principal components** - the columns of $\mathbf{U}$ - are the eigenvectors of $\mathbf{X}^T \mathbf{X}$.

(Equivalently, the eigenvectors of the **correlation matrix** - standardizing is automatic.)

### **Theorem**

-   Eigenvectors of $\mathbf{X}^T \mathbf{X}$ are orthogonal

-   Let $$
    \lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_p > 0
    $$ with eigenvectors $\mathbf{v}_1, \dots, \mathbf{v}_p$

    -   $\lambda_i$ is variance in direction $\mathbf{v}_i$
    -   $\mathbf{v}_1$ maximizes variance
    -   $\mathbf{v}_2$ maximizes variance subject to orthogonality with $\mathbf{v}_1$
    -   and so on

## Principal componponents regression (PC**R**) 

1.  Center and scale the data

2.  Form $\mathbf{U}$, the eigenvectors of centered and scaled $\mathbf{X}^T \mathbf{X}$, ordered by decreasing eigenvalues

3.  Compute $\mathbf{Z} = \mathbf{XU}$

4.  Select the number of components, $m$ based on explained variance (or cross-validation).

5.  Fit a linear regression using the first $m$ columns of $\mathbf{Z}$ as predictors.

::: {.callout-note appearance="simple"}
For $m = p$, PCR equals ordinary least squares
:::

:::: {.fragment}
In practice we will typically use software libraries to find the components
::: 

## Example: `wine` data (UCI ML Repository)

Chemical analysis of wines grown in the same region in Italy but derived from **three different cultivars**. The analysis determined the quantities of 13 constituents found in each of the **three types** of wines.

<br>

```{r}

wine <- read.csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data',
                 header = FALSE)

colnames(wine) <- c('Class_label', 'Alcohol', 'Malic_acid', 'Ash',
                    'Alcalinity_of_ash', 'Magnesium', 'Total_phenols',
                    'Flavanoids', 'Nonflavanoid_phenols', 'Proanthocyanins',
                    'Color_intensity', 'Hue',
                    'OD280_OD315_of_diluted_wines', 'Proline')

head(wine) %>% kable() %>%
  kable_styling(full_width = F, position = "left", font_size = 20)
```

```{r}


set.seed(0)
X <- as.matrix(wine[, -1]) %>% scale()
y <- wine[, 1]

```

```{r}
cormat <- cor(X)

eigen_decomp <- eigen(cormat)
eigen_vals <- eigen_decomp$values
eigen_vecs <- eigen_decomp$vectors

```

## Multinomial regression on the PCA features

We get perfect score with just 5 components, and 98% with only 3 components.

<br>

::::: columns
::: {.column width="50%"}
```{r}

pca_rec <- recipe(Class_label ~., data = wine) %>%
    step_normalize(all_predictors()) %>%
    step_impute_knn(all_predictors()) %>%
    step_pca(all_predictors())  

pca_prep <- prep(pca_rec)   

bake(pca_prep, wine) %>%
    ggplot(aes(PC1, PC2)) +
    geom_point(aes(color = as.factor(Class_label)), alpha = 0.7, size = 2) +
#theme_minimal() +
  theme(legend.position = "bottom") +
  labs(title = "Wine Class; First Two Principal Components",
      color = "Wine Class") +
    theme(
    axis.title.x = element_text(size = 18),
    axis.title.y = element_text(size = 18)
  ) 
```

```{r}
wine_pca <- prcomp(wine %>% select(-Class_label),
                    center = TRUE, scale = TRUE)

cumvar <- cumsum(wine_pca$sdev^2)/sum(wine_pca$sdev^2)

#print(cumvar[1:7])
#print(cumvar[8:13])

```
:::

::: {.column width="50%"}
```{r}

pc_features <- as.matrix(X) %*% eigen_vecs

X_pca <- pc_features
```

```{r}
facy <- as.factor(y)
multinom_pca <- function(num_comps = 13){
    X_pca_comps <- X_pca[, 1:num_comps, drop = FALSE]
    mod <- multinom_reg() %>%  fit(facy ~ ., data = data.frame(X_pca_comps))
    predictions <- predict(mod, new_data = data.frame(X_pca_comps))
    pred_augmented <- cbind(predictions, truth = facy) %>% as_tibble()
    score <- accuracy(pred_augmented, truth, .pred_class)
    
    acc_tbl <- accuracy(pred_augmented, truth, .pred_class)
    
    tibble(
        num_pca_components = num_comps,
        accuracy = acc_tbl$.estimate
    )
}
      

pca_metrics <- map_dfr(1:13, multinom_pca)

options(digits = 6)
pca_metrics %>% kable() %>%
  kable_styling(full_width = F, position = "left", font_size = 20)
```
:::
:::::

## PCA as a preprocessing step

<br>

-   on select predictors - not all or nothing.

-   preprocessing/feature engineering step for any ML model

-   Use `tidymodels::recipes`, `sklearn PCA`, `prcomp`

# Dimension reduction for visualization, understanding global structure, variable relationships {.small}

## PCA on `USArrests` dataset[^2]

<br>

[^2]: Base R datasets; McNeil, D. R. (1977) Interactive Data Analysis. New York: Wiley.

```{r}
USArrests <- USArrests |> mutate(state = rownames(USArrests))

head(USArrests %>% select(-state)) %>% kable() %>%
  kable_styling(full_width = F, position = "left", font_size = 20)

```

## Loadings and scores

::::: columns
::: {.column width="50%"}
<br>

PCA *loadings*; the $\mathbf{U}$ values

```{r}
USArrests <- USArrests |> mutate(state = rownames(USArrests))

arrests_pca <- prcomp(USArrests %>% select( -state),
                    center = TRUE, scale = TRUE)

arrests_pca$rotation %>% round(3) %>% kable() %>%
  kable_styling(full_width = F, position = "left", font_size = 18)
```

<br>

Cumulative normalized variances

```{r}

cumsum(arrests_pca$sdev^2)/sum(arrests_pca$sdev^2)

```
:::

::: {.column width="50%"}
<br>

PCA *scores*, $\mathbf{Z} = \mathbf{XU}$

```{r}
arrests_pca$x |> head(10) %>% kable() %>%
  kable_styling(full_width = F, position = "left", font_size = 18)
```
:::
:::::

## Visualizing the loadings

```{r}
pca_rec <- recipe(state ~., data = USArrests) %>%
  update_role(state, new_role = "id") %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors())

pca_prep <- prep(pca_rec)

```

```{r}
tidied_pca <- tidy(pca_prep, 2)

```

```{r}

tidied_pca %>%
  filter(component %in% paste0("PC", 1:4)) %>%
  group_by(component) %>%
  top_n(8, abs(value)) %>%
  ungroup() %>%
  mutate(terms = reorder_within(terms, abs(value), component)) %>%
  ggplot(aes(abs(value), terms, fill = value > 0)) +
  geom_col() +
  facet_wrap(~component, scales = "free_y") +
  scale_y_reordered() +
  labs(
    x = "Absolute value of contribution",
    y = NULL, fill = "Positive?"
  ) +
    theme(axis.text.x = element_blank())
```

```{r}
arrests_prep <- pca_rec |> prep() %>% bake(USArrests)
#arrests_prep |> head()
```

## Biplots

We can visualize the loadings and the scores together with a *biplot*.

```{r}
tidy_pca_wide <- tidied_pca %>% pivot_wider(names_from = component, values_from = value) 

```

```{r, echo = FALSE}
arrests_prep %>%
  ggplot(aes(PC1, PC2)) +
  geom_text(aes(label = state), check_overlap = TRUE, hjust = "inward") +
  geom_segment(
    data = tidy_pca_wide, aes(x = 0, y = 0, xend = 2 * PC1, yend = 2 * PC2),
    arrow = arrow(), color = "firebrick"
  ) +
  geom_text(
    data = tidy_pca_wide,
    aes(x = 2.3 * PC1, y = 1.8 * PC2, label = terms), color = "firebrick"
  ) +
  labs(color = NULL)
```

## Other matrix factorizations for feature extraction 

-   Non-negative Matrix Factorization (NNMF)

-   Independent Component Analysis (ICA)


# Non-linear dimension reduction

## Uniform manifold approximation and projection (UMAP)

<br> 

- [McInnes, L., Healy, J., & Melville, J. (2018)](https://arxiv.org/abs/1802.03426)

- UMAP python package: [umap-learn](https://umap-learn.readthedocs.io/en/latest/)

- R package: [`umap`](https://cran.r-project.org/web/packages/umap/index.html), can use `tidymodels` `recipes` step.

<br>

Constructs a graph representation of the data in high dimensions, then optimizes a low-dimensional graph to be as structurally similar as possible.

## Uniform manifold approximation and projection (UMAP)

`penguins` revisited

<br>

```{r}
penguins_filt <- palmerpenguins::penguins %>% select(-c(island, year, sex)) 

penguins_rec <- recipe(species ~., data = penguins_filt[-c(4, 272), ]) %>%
        step_normalize(all_predictors()) %>%
    step_impute_knn(all_predictors()) %>% 
    step_umap(all_predictors())

umap_prep <- prep(penguins_rec)

```

::::: columns
::: {.column width="50%"}
We use just the four numerical predictors

```{r}
penguins %>% head() %>% kable() %>%
  kable_styling(full_width = F, position = "left", font_size = 18)
```
:::

::: {.column width="50%"}
<br>

```{r}
bake(umap_prep, penguins_filt[-c(4, 272), ]) %>%
  ggplot(aes(UMAP1, UMAP2)) +
  geom_point(aes(color = as.factor(species)), alpha = 0.7, size = 2) +
  labs(color = NULL,
       title = "Projections of first two UMAP components by species") +
    theme(legend.position = "bottom")
```
:::
:::::

## PCA vs UMAP on `penguins`

<br>

::::: columns
::: {.column width="50%"}
```{r}
pca_rec <- recipe(species ~., data = penguins_filt[-c(4, 272), ]) %>%
        step_normalize(all_predictors()) %>%
    step_impute_knn(all_predictors()) %>%
    step_pca(all_predictors())  

pca_prep <- prep(pca_rec)   
```

```{r}
bake(pca_prep, penguins_filt[-c(4, 272), ]) %>%
  ggplot(aes(PC1, PC2)) +
  geom_point(aes(color = species), alpha = 0.7, size = 2) +
  labs(color = NULL,
       title = "Projections of first two PCA components by species") +
    theme(legend.position = "bottom")
```
:::

::: {.column width="50%"}
```{r}
bake(umap_prep, penguins_filt[-c(4, 272), ]) %>%
  ggplot(aes(UMAP1, UMAP2)) +
  geom_point(aes(color = as.factor(species)), alpha = 0.7, size = 2) +
  labs(color = NULL,
       title = "Projections of first two UMAP components by species") +
    theme(legend.position = "bottom")
```
:::
:::::

## UMAP may be overkill here

```{r}
ggpairs(penguins_filt, columns = c("bill_length_mm", "bill_depth_mm",
              "flipper_length_mm", "body_mass_g"), aes(color = species, alpha = 0.5)) +
    theme(axis.text.y = element_blank(), 
        axis.text.x = element_blank(), legend.position = "none") 
```

## UMAP

`mnist` dataset - images of handwritten digits (0-9). **784 features** (28 x 28 pixels)

```{r}

load(here::here("data/mnist_umap.RData"))
load(here::here("data/mnist_pca.RData"))

#mnist_rec <- recipe(class ~., data = mnist) %>%
  # step_umap(all_predictors())

#umap_prep <- prep(mnist_rec)

umap_mnist <- juice(umap_prep)

pca_mnist <- juice(mnist_pca_prep) 

```

<br>

::::: columns
::: {.column width="50%"}
![](images/mnist_extended_4_0.png){fig-alt="mnist samples"}
:::

::: {.column width="50%"}
```{r}
umap_mnist %>%
  ggplot(aes(UMAP1, UMAP2)) +
  geom_point(aes(color = as.factor(class)), alpha = 0.7, size = 2) +
  labs(color = NULL)
```
:::
:::::

## PCA vs UMAP on `mnist`

`mnist` dataset - images of handwritten digits (0-9). **784 features** (28 x 28 pixels)

<br>

::::: columns
::: {.column width="50%"}
```{r}
pca_mnist %>%
  ggplot(aes(PC1, PC2)) +
  geom_point(aes(color = as.factor(class)), alpha = 0.7, size = 2) +
  labs(color = NULL) +
    theme(legend.position = "none")
```
:::

::: {.column width="50%"}
```{r}
umap_mnist %>%
  ggplot(aes(UMAP1, UMAP2)) +
  geom_point(aes(color = as.factor(class)), alpha = 0.7, size = 2) +
  labs(color = NULL) +
    theme(legend.position = "none")
```
:::
:::::

## UMAP main hyperparameters {.incremental}

<br>

-   `n_neighbors` - number of neighboring points used in local approximations.

-   `min_dist` - minimum distance between points in low-dimensional space


These control the balance between local and global structure in the data.

## More on UMAP

-   [`umap-learn`](https://umap-learn.readthedocs.io/en/latest/) docs

-   A Coenen, A. Pearce blog [Understanding UMAP](https://pair-code.github.io/understanding-umap/)

## Comparing PCA, UMAP, and t-SNE 

![](images/PCA_UMAP_t-SNE.png){}
Source: [Biostatsquid](https://biostatsquid.com/pca-umap-tsne-comparison/)

# Neural Networks 

![](images/elephant-in-room.jpg){}

## Neural Networks

![](https://lamarr-institute.org/wp-content/uploads/deepLearn_2_EN.png){fig-alt="neural network diagaram"}

Source: [Gerhard Paab blog](https://lamarr-institute.org/blog/deep-neural-networks/)

## 

![](images/auto_encoder1.png){fig-alt="autoencoder diagaram" width="1672"} Source: [S. Raschka Deep Learning Learning](https://github.com/rasbt/stat453-deep-learning-ss20)

## Coding notes

-   `prcomp` in base R, `step_pca()` recipes step in `recipes` package (part of `tidymodels`)

-   `sklearn.decomposition.PCA`

-   `umap` package in R; as a `recipes()` step

-   `umap-learn` not part of `sklearn` but part of `sklearn` ecosystem

<br>

See my slides source code as well as example R and ipynb files for example code.

## Recap

-   **Why dimension reduction matters**

    -   High-dimensional predictors and rank deficiency
    -   Variance, multicollinearity, interpretability, computational cost
    -   The *curse of dimensionality* (analytical + simulation intuition)

-   **Feature selection, engineering, extraction**

-   **Principal Components Analysis (PCA)**

    -   Mathematics; eigenvectors and eigenvalues
    -   Principal components regression (PCR)
    -   PCA loadings and scores

-   **Nonlinear dimension reduction**

    -   UMAP, neural networks



