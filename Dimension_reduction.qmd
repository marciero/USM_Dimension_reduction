---
title: "Dimension Reduction"
subtitle: "A Quick Tour with Examples"
author: "USM Data Science"
date: "2026-02-03"
title-slide-attributes:
  data-background-color: "#002752"
format: 
    revealjs:
        footer: "Dimension Reduction/USM 2026-02-03"
        menu: false
        freeze: auto
        embed-resources: true
editor_options: 
  chunk_output_type: console
  
execute:
  echo: false
  warning: false
  message: false
  error: false
---

```{r echo=FALSE, warning=FALSE, include=FALSE}

library(tidyverse)
library(tidymodels)
library(palmerpenguins)
library(faraway)

library(broom)
library(GGally)
library(pscl)
library(embed)
library(patchwork)
library(kableExtra)
library(scales)
library(latex2exp)
library(tidytext)




```

```{r knitr, warning=FALSE}
options(htmltools.dir.version = FALSE, digits = 3)
knitr::opts_chunk$set(
  fig.width=7.5, fig.height=4.2, fig.retina=3,
  out.width = "100%",
  cache = FALSE,
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  fig.show = TRUE,
  hiline = TRUE,
  require("knitr")
)

theme_set(
  theme_minimal(base_size = 11) +
    theme(
      plot.title   = element_text(size = 20),
      axis.title   = element_text(size = 16),
      axis.text    = element_text(size = 14),
      legend.title = element_text(size = 16),
      legend.text  = element_text(size = 12)
    )
)
```

# What is Dimension Reduction?

## Consider a typical machine learning situation {.incremental}

::: fragment
-   one **response** variable (or none - unsupervised)
-   one or more **predictors** (features, variables)
:::

::: fragment
`penguins` data[^1].

```{r}
head(palmerpenguins::penguins) %>% kable() %>%
  kable_styling(full_width = F, position = "left", font_size = 20)

```
:::

[^1]: Palmer Archipelago, Antarctica. [Gorman, et. al, (2014)](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0090081), R package `palmerpenguins`, Horst et al.

:::: fragment
We are interested in the dimension of the predictor space.

::: {.callout-tip title="Think linear algebra"}
Dimension of the **column space**
:::
::::

## An Example {.incremental}

[Sakar et al. (2019)](https://doi.org/10.1016/j.asoc.2018.10.022)

-   252 patients, 188 of whom had a previous diagnosis of Parkinson’s disease, 64 without.
-   speech signal processing techniques yield 750 numerical predictors, or *features*.

<br>

Goal: classify Parkinson’s status.

<br>

:::: fragment
::: {.callout-note appearance="simple"}
Can we fit a logistic regression model?
:::
::::

## The `parkinsons` design matrix, $X$ {.incremental}

-   751 rows and 252 + 1 = 253 columns (one for the intercept)

-   More columns than rows!

-   So $X$ not *full rank* - a problem for linear models.

## Feature selection - one form of dimension reduction

Even if $X$ full rank, we might want eliminate some predictors to decrease model *variance*.

::: incremental
-   near - zero variance predictors

-   highly correlated predictors (with each other)

-   predictors uncorrelated with the response
:::

::: fragment
Many methods...
:::

::: incremental
-   Forward/backward stepwise selection
-   Lasso regression
-   ...
:::

## Another possibility: Feature engineering/extraction {.incremental}

Combine predictors into a smaller set of new predictors

:::::: fragment
::::: columns
::: {.column width="50%"}
```{r}

#ggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))

set.seed(123)
df <- tibble(x = runif(100, 0, 10)) %>% 
  mutate(y = x + rnorm(100, 0, 1)) 

df %>% 
ggplot(aes(x = x, y = y)) +
  geom_point() +
    geom_hline(yintercept = 0, linewidth = 1) +
  geom_vline(xintercept = 0, linewidth = 1) +
  theme(axis.text.y = element_blank(), 
        axis.text.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        #panel.grid = element_blank()
        )+
    labs(x = TeX("$X_1$"), 
       y = TeX("$X_2$"),
    title = "Two predictors living mostly in 1 D space")
```
:::

::: {.column width="50%"}
```{r}
df %>% 
ggplot(aes(x = x, y = y)) +
  geom_point() +
    geom_hline(yintercept = 0, linewidth = 1, alpha = 0.2) +
  geom_vline(xintercept = 0, linewidth = 1, alpha = 0.2) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed", linewidth = 1, color = "red") +
    geom_segment(
    x = 5, y = 5,
    xend = 4.5, yend = 6.5,
    linewidth = 1,
    color = "red",
    linetype = "dashed",
  ) +
  theme(axis.text.y = element_blank(), 
        axis.text.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        #panel.grid = element_blank()
        )+
    labs(x = NULL, 
       y = NULL,
    title = TeX("New features: $X_1 + X_2$ and $X_2 - X_1$"))
```
:::
:::::
::::::

:::::: fragment
::::: columns
::: {.column width="50%"}
```{r}

#ggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))

set.seed(123)
df <- tibble(x = runif(100, 0, 10)) %>% 
  mutate(y = (x - 5)^2  + rnorm(100, 0, 3)) %>% 
    mutate(xsq = x^2  + rnorm(100, 0, 3))

df %>% 
ggplot(aes(x = x, y = y)) +
  geom_point() +
    geom_hline(yintercept = 0, linewidth = 1) +
  geom_vline(xintercept = 0, linewidth = 1) +
  theme(axis.text.y = element_blank(), 
        axis.text.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        #panel.grid = element_blank()
        )+
    labs(x = TeX("$X_1$"), 
       y = TeX("$X_2$"), 
    title = "Two predictors living mostly in 1 D space")
```
:::

::: {.column width="50%"}
```{r}
df %>% 
ggplot(aes(x = x, y = xsq)) +
  geom_point() +
    geom_hline(yintercept = 0, linewidth = 1, alpha = 0.2) +
  geom_vline(xintercept = 0, linewidth = 1, alpha = 0.2) +
    geom_smooth(method = "lm", se = FALSE, linewidth = 1, linetype = "dashed") +
  theme(axis.text.y = element_blank(), 
        axis.text.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        #panel.grid = element_blank()
        )+
    labs(x = NULL, 
       y = NULL,
       title = TeX("New feature: $X_1^2$"))
  
```
:::
:::::
::::::

:::: fragment
::: {callout-tip}
More generally, can we transform and combine predictors in various ways to form a smaller set of new *features?*
:::
::::

## Why do dimension reduction? {.incremental}

::: incremental
-   Reduce model variance
-   Reduce model complexity
-   Improve model interpretability
-   Reduce computational cost
-   Mitigate multicollinearity
-   Mitigate the **"curse of dimensionality"**
:::

:::: fragment
::: {.callout-note appearance="simple"}
Much of this is model dependent
:::
::::

::: fragment
Also used for

-   Visualize high-dimensional data
-   data compression
-   autoencoders - representation learning
:::

:::: fragment
::: {.callout-note appearance="simple"}
These last three are **unsupervised**!
:::
::::

# Curse of dimensionality - simulation example

[need example with cat variable lots of levels ]

skimr::skim(ames)

ames_prep <- recipe(Sale_Price ~ ., ames) %>%
    step_dummy(all_nominal()) %>% 
    prep() %>% bake(ames)

skimr::skim(ames_prep)



hotels <-
    read_csv("https://tidymodels.org/start/case-study/hotels.csv") 


hotel_prep <- recipe(children ~ ., hotels) %>%
    step_dummy(all_nominal_predictors()) %>% 
    prep() %>% bake(hotels)
skimr::skim(hotel_prep)


## Curse of dimensionality - simple analytical example

Suppose KNN with $p$ predictors $\sim U(0,1)$

-   volume of hypercube with sides of length $d$ is $d^p$.

-   To capture $r$ proportion of the data, we want $d^p = r$, so $d = r^{1/p}$

```{r}
p <- c(1:100)
r <- c(0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 0.5)


df <- expand.grid(p = p, r = r) %>%
    mutate(d = r^(1/p),
           r_lab = if_else(r < 0.1,
                           paste0("10^{", format(log10(r), trim = TRUE), "}"),
                           as.character(r))) %>%
    mutate(r_lab = factor(r_lab, levels = unique(r_lab)))  # preserve order

ggplot(df, aes(x = p, y = d, color = r_lab)) +
    geom_line() +
    geom_point() +
    labs(x = "Number of predictors, p", y = "Box side length, d") +
    scale_color_discrete(name = "Proportion of data, r", labels = label_parse()) +
    theme_minimal()

```

::: {.callout-note appearance="simple"}
We are using the $L_{\infty}$ norm (max norm) to define neighborhood here, but argument is similar for other norms.
:::

## Features

-   predictors or variables used in a model

-   transformed and combined versions of the original predictors

**Feature engineering** is the art of creating useful features; it may increase or decrease the number of predictors.

## Dimension reduction

*Reducing the number of features/dimension of feature space*

-   Feature selection - select a subset of the original predictors

-   Feature extraction - create new predictors from the original ones

    -   **PCA**, **UMAP**, tSNE, neural networks/deep learning

# Feature extraction

*Transforming and combining predictors to form new "features"*

# Canonical example: **Principal Components Analysis (PCA)**

## Principal Components Analysis (PCA)

Create features - *components* from **linear** combinations of the predictors

::::: columns
::: {.column width="50%"}
```{r}

#ggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))

set.seed(123)
df <- tibble(x = runif(100, 0, 10)) %>% 
  mutate(y = x + rnorm(100, 0, 1)) 

df %>% 
ggplot(aes(x = x, y = y)) +
  geom_point() +
    geom_hline(yintercept = 0, linewidth = 1) +
  geom_vline(xintercept = 0, linewidth = 1) +
  theme(axis.text.y = element_blank(), 
        axis.text.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        #panel.grid = element_blank()
        )+
    labs(x = "X1", 
       y = "X2",
    title = "Two predictors living mostly in 1 D space")
```
:::

::: {.column width="50%"}
```{r}
df %>% 
ggplot(aes(x = x, y = y)) +
  geom_point() +
    geom_hline(yintercept = 0, linewidth = 1, alpha = 0.2) +
  geom_vline(xintercept = 0, linewidth = 1, alpha = 0.2) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed", linewidth = 1, color = "red") +
    geom_segment(
    x = 5, y = 5,
    xend = 4.5, yend = 6.5,
    linewidth = 1,
    color = "red",
    linetype = "dashed",
  ) +
  theme(axis.text.y = element_blank(), 
        axis.text.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        #panel.grid = element_blank()
        )+
    labs(x = NULL, 
       y = NULL,
    title = "New features: aX1 + bX2 and cX2 - dX1")
```
:::
:::::

-   The first component is in the direction of maximum variance of the data\*.

-   Second component in direction of max variance orthogonal (and this uncorrelated) to the first, and so on.

::: {.callout-note appearance="simple"}
\*It thus makes sense to standardize predictors first if they are on different scales.
:::

## PCA - the math

Predictors $X_1, X_2, \dots, X_p$; form linear combinations

$$
Z_m = \sum_{j=1}^p \phi_{jm} X_j,
\qquad m = 1, 2, \dots, p
$$

In matrix form,

$$
Z = X \mathbf{U}
$$

## Now with data

$$
\mathbf{Z} = \mathbf{XU}
$$

## Principal components and eigenvectors

First scale or standardize the data matrix.

The **principal components** (columns of $\mathbf{U}$) are the eigenvectors of $\mathbf{X}^T \mathbf{X}$.

(Equivalently, the eigenvectors of the correlation matrix - and dont need to standardize.)

#### **Theorem**

-   Eigenvectors of $\mathbf{X}^T \mathbf{X}$ are orthogonal

-   Let $$
    \lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_p > 0
    $$ with eigenvectors $\mathbf{v}_1, \dots, \mathbf{v}_p$

    -   $\lambda_i$ is variance in direction $\mathbf{v}_i$
    -   $\mathbf{v}_1$ maximizes variance
    -   $\mathbf{v}_2$ maximizes variance subject to orthogonality with $\mathbf{v}_1$
    -   and so on

## Principal components regression (PCR) in practice

1.  Center and scale the data

2.  Form $\mathbf{U}$, the eigenvectors of the centered and scaled $\mathbf{X}^T \mathbf{X}$, ordered by decreasing eigenvalues

3.  Compute $\mathbf{Z} = \mathbf{XU}$

4.  Select the number of components based on explained variance (or cross-validation)

5.  Fit a linear regression using the first $m$ components

::: {.callout-note appearance="simple"}
For $m = p$, PCR equals ordinary least squares
:::

## Example: `meatspec` data

#### Analytical chemistry - predict fat content of meat samples from 100 channel infrared absorbance spectra.

```{r}
fit <- lm(fat ~ ., meatspec)



x<- as.matrix(meatspec[, 1:100])   %>% scale()
XX <- t(x) %*% x

## Eigen decomp
XXeig <- eigen(XX)


## We find the cumulative percent variation in the components as in class
total_XXvar <- XXeig$values %>% sum()
cum_var <- XXeig$values %>% cumsum()/total_XXvar %>% head()


## Fit the model with eight components and find the rmse
Z <- x %*% XXeig$vectors
fitZ <- lm(meatspec$fat ~ Z[, 1:8])



```

PCA components cumulative variances

```{r}
options(digits = 5)
cum_var[1:10] %>% round(5)
```

::::: columns
::: {.column width="50%"}
Full model performance

```{r}
cbind(preds = predict(fit, meatspec), truth = meatspec$fat) %>% as_tibble %>% metrics(truth, preds) %>% 
    transmute(metric = .metric, estimate = .estimate) %>%
  kable() %>%
  kable_styling(full_width = F, position = "left", font_size = 20)
```
:::

::: {.column width="50%"}
PCR with 8 components performance

```{r}


cbind(preds = predict(fitZ), truth = meatspec$fat) %>% as_tibble %>% metrics(truth, preds) %>% 
    transmute(metric = .metric, estimate = .estimate) %>%
  kable() %>%
  kable_styling(full_width = F, position = "left", font_size = 20)

```
:::
:::::

## R squared vs RMSE

![Observed versus predicted values for models that are optimized using the RMSE compared to the coefficient of determination](https://www.tmwr.org/figures/performance-reg-metrics-1.png)

Source: [Tidy Modeling with R](https://www.tmwr.org/), Max Kuhn and Julia Silge, 2023

## Example: `wine` data (UCI ML Repository)

Chemical analysis of wines grown in the same region in Italy but derived from **three different cultivars**. The analysis determined the quantities of 13 constituents found in each of the \*\*three types\* of wines.

```{r}

wine <- read.csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data',
                 header = FALSE)

colnames(wine) <- c('Class_label', 'Alcohol', 'Malic_acid', 'Ash',
                    'Alcalinity_of_ash', 'Magnesium', 'Total_phenols',
                    'Flavanoids', 'Nonflavanoid_phenols', 'Proanthocyanins',
                    'Color_intensity', 'Hue',
                    'OD280_OD315_of_diluted_wines', 'Proline')

head(wine) %>% kable() %>%
  kable_styling(full_width = F, position = "left", font_size = 20)
```

```{r}


set.seed(0)
X <- as.matrix(wine[, -1]) %>% scale()
y <- wine[, 1]

```

```{r}
cormat <- cor(X)

eigen_decomp <- eigen(cormat)
eigen_vals <- eigen_decomp$values
eigen_vecs <- eigen_decomp$vectors

```

## Total and explained variance of the PCA components

```{r}
tot <- sum(eigen_vals)
var_exp <- sort(eigen_vals, decreasing = TRUE) / tot
cum_var_exp <- cumsum(var_exp)
```

```{r}

variance_df <- data.frame(
  PC = 1:13,
  Individual = var_exp,
  Cumulative = cum_var_exp
)

ggplot(variance_df, aes(x = PC)) +
  geom_bar(aes(y = Individual), stat = "identity", fill = "steelblue", 
           alpha = 0.7) +
  geom_step(aes(y = Cumulative), color = "darkred", linewidth = 1) +
  labs(y = "Explained variance ratio", 
       x = "Principal component") +
  theme_minimal()
```

## Multinomial regression on the PCA features

<br>

We get perfect score with just 5 components, and 98% with only 3 components.

<br>

::::: columns
::: {.column width="50%"}
```{r}

pca_rec <- recipe(Class_label ~., data = wine) %>%
    step_normalize(all_predictors()) %>%
    step_impute_knn(all_predictors()) %>%
    step_pca(all_predictors())  

pca_prep <- prep(pca_rec)   

bake(pca_prep, wine) %>%
    ggplot(aes(PC1, PC2)) +
    geom_point(aes(color = as.factor(Class_label)), alpha = 0.7, size = 2) +
#theme_minimal() +
  theme(legend.position = "bottom") +
  labs(title = "Wine Class; First Two Principal Components",
      color = "Wine Class") +
    theme(
    axis.title.x = element_text(size = 18),
    axis.title.y = element_text(size = 18)
  ) 
```
:::

::: {.column width="50%"}
```{r}

pc_features <- as.matrix(X) %*% eigen_vecs

X_pca <- pc_features
```

```{r}
facy <- as.factor(y)
multinom_pca <- function(num_comps = 13){
    X_pca_comps <- X_pca[, 1:num_comps, drop = FALSE]
    mod <- multinom_reg() %>%  fit(facy ~ ., data = data.frame(X_pca_comps))
    predictions <- predict(mod, new_data = data.frame(X_pca_comps))
    pred_augmented <- cbind(predictions, truth = facy) %>% as_tibble()
    score <- accuracy(pred_augmented, truth, .pred_class)
    
    acc_tbl <- accuracy(pred_augmented, truth, .pred_class)
    
    tibble(
        num_pca_components = num_comps,
        accuracy = acc_tbl$.estimate
    )
}
      

pca_metrics <- map_dfr(1:13, multinom_pca)

options(digits = 6)
pca_metrics %>% kable() %>%
  kable_styling(full_width = F, position = "left", font_size = 20)
```
:::
:::::

## Loadings and scores

::::: columns
::: {.column width="50%"}
<br>

PCA *loadings*; the $\mathbf{U}$ values

```{r}

wine_pca <- prcomp(wine %>% select(-Class_label),
                    center = TRUE, scale = TRUE)

wine_pca$rotation %>% round(3) %>% kable() %>%
  kable_styling(full_width = F, position = "left", font_size = 18)
```

<br>

Cumulative normalized variances

```{r}

cumsum(wine_pca$sdev^2)/sum(wine_pca$sdev^2)

```
:::

::: {.column width="50%"}
<br>

PCA *scores*, $\mathbf{Z} = \mathbf{XU}$

```{r}
wine_pca$x |> head(10) %>% kable() %>%
  kable_styling(full_width = F, position = "left", font_size = 18)
```
:::
:::::

::: {.callout-warning}
These are "unscaled" loadings and scores. The `prcomp` function in R uses scaling by default.

## Visualizing the loadings

First four PCAcomponents

```{r}

pca_rec <- recipe(Class_label~., data = wine) %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors())

pca_prep <- prep(pca_rec)
```

```{r, echo = FALSE}
tidied_pca <- tidy(pca_prep, 2)

tidied_pca %>%
     filter(component %in% paste0("PC", 1:4)) %>% 
 mutate(component = fct_inorder(component)) %>%
  ggplot(aes(value, terms)) +
  geom_col(fill = "steelblue", show.legend = FALSE) +
  facet_wrap(~component, nrow = 1) +
    theme(axis.text.x = element_blank()) +
  labs(y = NULL, x = NULL) 
```

## Example: `USArrests` dataset[^2] 

[^2]: Base R datasets; McNeil, D. R. (1977) Interactive Data Analysis. New York: Wiley.

```{r}
USArrests <- USArrests |> mutate(state = rownames(USArrests))

head(USArrests %>% select(-state)) %>% kable() %>%
  kable_styling(full_width = F, position = "left", font_size = 20)

```

## Loadings and scores

::::: columns
::: {.column width="50%"}
<br>

PCA *loadings*; the $\mathbf{U}$ values

```{r}
USArrests <- USArrests |> mutate(state = rownames(USArrests))

arrests_pca <- prcomp(USArrests %>% select( -state),
                    center = TRUE, scale = TRUE)

arrests_pca$rotation %>% round(3) %>% kable() %>%
  kable_styling(full_width = F, position = "left", font_size = 18)
```

<br>

Cumulative normalized variances

```{r}

cumsum(arrests_pca$sdev^2)/sum(arrests_pca$sdev^2)

```
:::

::: {.column width="50%"}
<br>

PCA *scores*, $\mathbf{Z} = \mathbf{XU}$

```{r}
arrests_pca$x |> head(10) %>% kable() %>%
  kable_styling(full_width = F, position = "left", font_size = 18)
```
:::
:::::

## Visualizing the feature contributions in the components

```{r}
pca_rec <- recipe(state ~., data = USArrests) %>%
  update_role(state, new_role = "id") %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors())

pca_prep <- prep(pca_rec)

```

```{r}
tidied_pca <- tidy(pca_prep, 2)

```

```{r}

tidied_pca %>%
  filter(component %in% paste0("PC", 1:4)) %>%
  group_by(component) %>%
  top_n(8, abs(value)) %>%
  ungroup() %>%
  mutate(terms = reorder_within(terms, abs(value), component)) %>%
  ggplot(aes(abs(value), terms, fill = value > 0)) +
  geom_col() +
  facet_wrap(~component, scales = "free_y") +
  scale_y_reordered() +
  labs(
    x = "Absolute value of contribution",
    y = NULL, fill = "Positive?"
  ) +
    theme(axis.text.x = element_blank())
```

```{r}
arrests_prep <- pca_rec |> prep() %>% bake(USArrests)
#arrests_prep |> head()
```

## Biplots

We can visualize the loadings and the scores together with a *biplot*.

```{r}
tidy_pca_wide <- tidied_pca %>% pivot_wider(names_from = component, values_from = value) 

```

```{r, echo = FALSE}
arrests_prep %>%
  ggplot(aes(PC1, PC2)) +
  geom_text(aes(label = state), check_overlap = TRUE, hjust = "inward") +
  geom_segment(
    data = tidy_pca_wide, aes(x = 0, y = 0, xend = 2 * PC1, yend = 2 * PC2),
    arrow = arrow(), color = "firebrick"
  ) +
  geom_text(
    data = tidy_pca_wide,
    aes(x = 2.3 * PC1, y = 1.8 * PC2, label = terms), color = "firebrick"
  ) +
  labs(color = NULL)
```

# Non-linear dimension reduction

UMAP, t-SNE, autoencoders (neural networks)

## UMAP: `penguins` revisited

```{r}
penguins_filt <- penguins %>% select(-c(island, year, sex)) 

penguins_rec <- recipe(species ~., data = penguins_filt[-c(4, 272), ]) %>%
        step_normalize(all_predictors()) %>%
    step_impute_knn(all_predictors()) %>% 
    step_umap(all_predictors())

umap_prep <- prep(penguins_rec)

```

::::: columns
::: {.column width="50%"}
We use just the four numerical predictors

```{r}
penguins %>% head() %>% kable() %>%
  kable_styling(full_width = F, position = "left", font_size = 18)
```
:::

::: {.column width="50%"}
```{r}
bake(umap_prep, penguins_filt[-c(4, 272), ]) %>%
  ggplot(aes(UMAP1, UMAP2)) +
  geom_point(aes(color = as.factor(species)), alpha = 0.7, size = 2) +
  labs(color = NULL,
       title = "Projections of first two UMAP components by species") +
    theme(legend.position = "bottom")
```
:::
:::::

## PCA vs UMAP on `penguins`

::::: columns
::: {.column width="50%"}
```{r}
pca_rec <- recipe(species ~., data = penguins_filt[-c(4, 272), ]) %>%
        step_normalize(all_predictors()) %>%
    step_impute_knn(all_predictors()) %>%
    step_pca(all_predictors())  

pca_prep <- prep(pca_rec)   
```

```{r}
bake(pca_prep, penguins_filt[-c(4, 272), ]) %>%
  ggplot(aes(PC1, PC2)) +
  geom_point(aes(color = species), alpha = 0.7, size = 2) +
  labs(color = NULL,
       title = "Projections of first two PCA components by species") +
    theme(legend.position = "bottom")
```
:::

::: {.column width="50%"}
```{r}
bake(umap_prep, penguins_filt[-c(4, 272), ]) %>%
  ggplot(aes(UMAP1, UMAP2)) +
  geom_point(aes(color = as.factor(species)), alpha = 0.7, size = 2) +
  labs(color = NULL,
       title = "Projections of first two UMAP components by species") +
    theme(legend.position = "bottom")
```
:::
:::::

## Penguins pairplot

UMAP may be overkill here

```{r}
ggpairs(penguins_filt, columns = c("bill_length_mm", "bill_depth_mm",
              "flipper_length_mm", "body_mass_g"), aes(color = species, alpha = 0.5)) +
    theme(axis.text.y = element_blank(), 
        axis.text.x = element_blank(), legend.position = "none") 
```

## UMAP

#### `mnist` dataset[^3] - images of handwritten digits (0-9). **784 features** (28 x 28 pixels)

[^3]: [National Institute of Standards and Technology](https://en.wikipedia.org/wiki/MNIST_database), 1988

::::: columns
::: {.column width="50%"}
![MNIST handwritten digits](https://storage.googleapis.com/kaggle-datasets-images/6967763/11165648/7c42982e904922bea475ad399ded5ae3/dataset-cover.png?t=2025-03-25-20-33-29)
:::

::: {.column width="50%"}
```{r}

load(here::here("data/mnist_umap.RData"))

#mnist_rec <- recipe(class ~., data = mnist) %>%
  # step_umap(all_predictors())

#umap_prep <- prep(mnist_rec)


```

```{r}
juice(umap_prep) %>%
  ggplot(aes(UMAP1, UMAP2)) +
  geom_point(aes(color = as.factor(class)), alpha = 0.7, size = 2) +
  labs(color = NULL)
```
:::
:::::

## PCA vs UMAP on `mnist`

::::: columns
::: {.column width="50%"}
```{r}

#target <- read_csv(here::here("../../Courses/DSC260/data/mnist_target.csv"))
#mnist_data <- read_csv(here::here("../../Courses/DSC260/data/mnist.csv"))/255
#mnist <- mnist %>% bind_cols(target)

# mnist_rec <- recipe(class ~., data = mnist) %>%
#  step_pca(all_predictors())
# mnist_pca_prep <- prep(mnist_rec)


load(here::here("data/mnist_pca.RData"))

```

```{r}
juice(mnist_pca_prep) %>%
  ggplot(aes(PC1, PC2)) +
  geom_point(aes(color = as.factor(class)), alpha = 0.7, size = 2) +
  labs(color = NULL) +
    theme(legend.position = "none")
```
:::

::: {.column width="50%"}
```{r}
juice(umap_prep) %>%
  ggplot(aes(UMAP1, UMAP2)) +
  geom_point(aes(color = as.factor(class)), alpha = 0.7, size = 2) +
  labs(color = NULL) +
    theme(legend.position = "none")
```
:::
:::::

## Neural Networks for Dimension Reduction

![](https://lamarr-institute.org/wp-content/uploads/deepLearn_2_EN.png){fig-alt="neural network diagaram"}

Source: [Gerhard Paab blog](https://lamarr-institute.org/blog/deep-neural-networks/)

## What we covered

-   **Why dimension reduction matters**

    -   High-dimensional predictors and rank deficiency
    -   Variance, multicollinearity, and computational cost
    -   The *curse of dimensionality* (analytical + simulation intuition)

-   **Two broad strategies**

    -   **Feature selection**:
    -   **Feature extraction**:

-   **Principal Components Analysis (PCA)**

-   **Class experiments/examples**

    -   `meatspec`: PCR with many correlated predictors
    -   `wine`: classification with few principal components
    -   `USArrests`: interpreting loadings, scores, and biplots
    -   `penguins`: PCA vs UMAP for visualization

-   **Nonlinear dimension reduction**

    -   UMAP, neural networks
